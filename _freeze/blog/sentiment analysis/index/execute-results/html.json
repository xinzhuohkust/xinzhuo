{
  "hash": "ffe5b4c2c994b80857ca733b1bafebd3",
  "result": {
    "markdown": "---\ntitle: \"Sentiment Analysis\"\ndate: \"last-modified\"\ncategories: [R, Python, Sentiment Analysis, Reticulate]\ndescription: \"Text analysis with Python and R\"\ncode-fold: show\n---\n\n\n![](images/sentiment.png)\n\nText analysis enables the identification and extraction of sentiment information from text. By leveraging tools such as `asent`, `sentimentr`, and basic sentiment lexicons, it is possible to construct a sentiment classifier that can estimate whether the underlying sentiment of a given text is positive, negative, or neutral.\n\n# Pacakage Management\n\nUsing `pacman` to manage package dependencies.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(pacman)\np_load(dplyr, magrittr, furrr, purrr, DT, readr, stringr, furrr, purrr, sentimentr, tidyfst, tidyr, textclean, tidytext)\n```\n:::\n\n\n# Load Data\n\nOur demonstration data are sourced from a news dataset collected from Factiva, consisting of 500 observations. The \"link\" refers to the links of these news articles. These news articles have been translated into English and undergone thorough text cleaning. Please find the data [here.](https://drive.google.com/file/d/1JKj8rDatjionAYN1EhzFnSNBRNQkay0C/view){.external target=\"_blank\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_rds(\"E:/OneDrive - HKUST Connect/SOSC/paper_with_jean/group_meeting/fifth/data_for_analysis_share.Rds\")\nglimpse(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 500\nColumns: 2\n$ link         <chr> \"https://global.factiva.com/redir/default.aspx?P=sa&an=AF…\n$ cleaned_text <chr> \"the head of the women tennis association says that he is…\n```\n:::\n:::\n\n\n# Using asent\n\nTo use `asent`, a Python package in R, you need `reticulate` package to integrate Python code in your R script. You can change your python interpreter here:\n\n![](images/python_in_R.png){width=\"550\"}\n\n## install python packages:\n\nCall python in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreticulate::use_python(\"C:\\\\Users\\\\xhuangcb\\\\anaconda3\\\\envs\\\\pytorch_gpu\\\\python.exe\") # call python in R\n```\n:::\n\n\nTyping`!pip install spacy` to install `spacy`. Typing `!pip install asent\"` to install `asent`. Lastly, typing `!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl` to install pre-trained model.\n\n![](images/callpythoninr.png){width=\"550\"}\n\n## from OOP to FP\n\nPython is an object-oriented programming (OOP) language, and it is possible to convert Python classes into functions in R, which mainly support functional programming (FP) language.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspacy <- reticulate::import(\"spacy\") # load spacy\nasent <- reticulate::import(\"asent\") # load asent\nnlp <- spacy$load(\"en_core_web_lg\") # load pre-trained model\nnlp$add_pipe(\"asent_en_v1\") # add asent pipe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<asent.component.Asent object at 0x00000168525BA9D0>\n```\n:::\n\n```{.r .cell-code}\nnlp_safe <- possibly(.f = nlp, otherwise = \"error!\") # error handling\nget_polarity <- nlp_safe(\"sentiment\")$get_extension(\"polarity\")[[3]] # load python function\nget_sentiment_asent <- function(x) {\n    result <- nlp_safe(x) %>%\n        get_polarity() %>%\n        print() %>%\n        capture.output() %>%\n        str_extract_all(\"-?0\\\\.\\\\d+\") %>%\n        unlist() %>%\n        as.numeric()\n\n    names(result) <- c(\"neg\", \"neu\", \"pos\", \"compound\")\n\n    return(result)\n}\n```\n:::\n\n\nTry our function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiment_asent(data$cleaned_text[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     neg      neu      pos compound \n  0.0440   0.6350   0.0710   0.1235 \n```\n:::\n:::\n\n\nIt is an analytical process of asent:\n\n![](images/result-01.png){width=\"680\"}\n\n# Using sentimentr\n\nOur function using `sentimentr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentences_safe <- possibly(.f = get_sentences, otherwise = \"error!\") # sentences cutter\nsentiment_by_safe <- possibly(.f = sentiment_by, otherwise = \"error!\") # sentiment classifier\nget_sentiemnt_sentimentr <- function(x) {\n    x %>%\n        get_sentences_safe() %>%\n        sentiment_by_safe() %>%\n        unlist()\n}\n```\n:::\n\n\nTry it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiemnt_sentimentr(data$cleaned_text[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   element_id    word_count            sd ave_sentiment \n    1.0000000   382.0000000     0.3115586     0.1770496 \n```\n:::\n:::\n\n\n# Word-level\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncal_sentiment <- function(x) {\n    x %>%\n        filter(value == 0) %>%\n        pull(value) %>%\n        length() -> neu\n\n    x %>%\n        filter(value > 0) %>%\n        pull(value) %>%\n        sum() -> pos\n\n    x %>%\n        filter(value < 0) %>%\n        pull(value) %>%\n        sum() -> neg\n    result <- c(neu, pos, neg)\n\n    names(result) <- c(\"neu\", \"pos\", \"neg\")\n\n    return(result)\n}\nget_sentiment_word <- function(x) {\n    result <- x %>%\n        tibble(text = ., id = 1) %>%\n        unnest_tokens(word, text) %>%\n        filter(!word %in% stopwords::stopwords(source = \"stopwords-iso\")) %>%\n        left_join(get_sentiments(\"afinn\")) %>% # you can try different dict\n        replace_na(list(value = 0)) %>%\n        nest(data = !id) %>%\n        mutate(sentiment = map(data, cal_sentiment)) %>%\n        pull(sentiment) %>%\n        unlist()\n\n    return(result)\n}\n```\n:::\n\n\nTry it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiment_word(data$cleaned_text[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nneu pos neg \n138  18 -12 \n```\n:::\n:::\n\n\n# Use these functions in loop\n\nYou can use these function with `for` loop or just use `map`to apply a them to each element of a vector or list in tibble.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data %>%\n    mutate(sentiment_asent = map(cleaned_text, get_sentiment_asent, .progress = TRUE))\n```\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}